{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = os.getcwd()\n",
    "p = os.path.dirname(d)\n",
    "\n",
    "sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dynamics import RobotWorld\n",
    "from src.algorithms.addpgpd_sampled import ADpgpdSampled\n",
    "from src.algorithms.pgdual import LinearDual\n",
    "from src.sampling import Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 4\n",
    "da = 2\n",
    "\n",
    "tau = 0.01\n",
    "gamma = 0.95\n",
    "b = - 200.0\n",
    "\n",
    "G = - torch.tensor([\n",
    "    [1.0, 0, 0, 0],\n",
    "    [0, 1.0, 0, 0],\n",
    "    [0, 0, 0.1, 0],\n",
    "    [0, 0, 0, 0.1]\n",
    "]).double()\n",
    "\n",
    "R =  - torch.tensor([\n",
    "    [0.1, 0],\n",
    "    [0, 0.1],\n",
    "]).double()\n",
    "\n",
    "def primal_reward_fn(env, a):\n",
    "    return ((env.s @ G) * env.s).sum(dim=1) + ((a @ R) * a).sum(dim=1)\n",
    "\n",
    "def primal_reward_reg_fn(env, a):\n",
    "    return - (tau / 2) * (a * a).sum(dim=1)\n",
    "\n",
    "def dual_reward_fn(env, a):\n",
    "    return 100 * (env.s[:, 0].clip(max=1.0) + env.s[:, 1].clip(max=1.0) - 2)\n",
    "\n",
    "def starting_pos_fn(nsamples):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    s = torch.tensor(rng.uniform(\n",
    "        low=[40, 40, -10, -10],\n",
    "        high= [50, 50, 10, 10],\n",
    "        size=[nsamples, 4],\n",
    "    )).double()\n",
    "\n",
    "    a = torch.tensor(rng.uniform(\n",
    "        low=[-10, -10],\n",
    "        high= [10, 10],\n",
    "        size=[nsamples, 2],\n",
    "    )).double()\n",
    "\n",
    "    return s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - A-DPPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - Unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1_000\n",
    "n_pe = 100\n",
    "n_rho = 2_000\n",
    "n_roll = 200\n",
    "\n",
    "alpha = 1.0\n",
    "eta = 0.001\n",
    "\n",
    "env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "sampler = Sampler(env, gamma)\n",
    "dpgpd = ADpgpdSampled(ds, da, env, eta, tau, gamma, b, alpha, primal_reward_fn, primal_reward_reg_fn, dual_reward_fn, starting_pos_fn)\n",
    "\n",
    "K, losses_primal, losses_dual = dpgpd.train_unconstrained(epochs, n_pe, n_rho, n_roll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B - Constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50_000\n",
    "n_pe = 100\n",
    "n_rho = 2_000\n",
    "n_roll = 200\n",
    "\n",
    "alpha = 1.0\n",
    "eta = 0.00005\n",
    "\n",
    "env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "sampler = Sampler(env, gamma)\n",
    "dpgpd = ADpgpdSampled(ds, da, env, eta, tau, gamma, b, alpha, primal_reward_fn, primal_reward_reg_fn, dual_reward_fn, starting_pos_fn)\n",
    "\n",
    "K, lmbda, losses_primal, losses_dual = dpgpd.train_constrained(epochs, n_pe, n_rho, n_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../results/obs_primal.npy', losses_primal)\n",
    "np.save('../results/obs_dual.npy', losses_dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - PGDual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50_000\n",
    "n_samples = 100\n",
    "n_rollout = 200\n",
    "n_rho = 2_000\n",
    "\n",
    "n_dual_update = 10\n",
    "lr_actor = 1e-4\n",
    "lr_dual = 1e-3\n",
    "\n",
    "env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "\n",
    "ld = LinearDual(ds, da, env, lr_actor, lr_dual, gamma, b, starting_pos_fn, primal_reward_fn, dual_reward_fn)\n",
    "loss_primal, loss_dual = ld.train(n_epochs, n_samples, n_rollout, n_rho, n_dual_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../results/obs_primal_dm.npy', loss_primal)\n",
    "np.save('../results/obs_dual_dm.npy', loss_dual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
