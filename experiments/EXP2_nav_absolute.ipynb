{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = os.getcwd()\n",
    "p = os.path.dirname(d)\n",
    "\n",
    "sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dynamics import RobotWorld\n",
    "from src.algorithms.addpgpd_sampled import ADpgpdSampled\n",
    "from src.algorithms.pgdual import LinearDual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 4\n",
    "da = 2\n",
    "\n",
    "b = - 1_000\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.2\n",
    "\n",
    "G1 = - torch.tensor([1.0, 1.0, .001, .001]).double()\n",
    "G2 = - torch.tensor([.001, .001, 1.0, 1.0]).double()\n",
    "\n",
    "R1 = - torch.tensor([0.01, 0.01]).double()\n",
    "R2 = - torch.tensor([0.01, 0.01]).double()\n",
    "\n",
    "def primal_reward_fn(env, a):\n",
    "    return (env.s.abs() * G1).sum(dim=1) + (a.abs() * R1).sum(dim=1)\n",
    "\n",
    "def primal_reward_reg_fn(env, a):\n",
    "    return - (tau / 2) * (a * a).sum(dim=1)\n",
    "\n",
    "def dual_reward_fn(env, a):\n",
    "    return ((env.s ** 2) * G2).sum(dim=1) + (tau / 2) + ((a ** 2) * R2).sum(dim=1)\n",
    "\n",
    "def starting_pos_fn(nsamples):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    s = torch.tensor(rng.uniform(\n",
    "        low=[40, 40, -10, -10],\n",
    "        high= [50, 50, 10, 10],\n",
    "        size=[nsamples, 4],\n",
    "    )).double()\n",
    "\n",
    "    a = torch.tensor(rng.uniform(\n",
    "        low=[-10, -10],\n",
    "        high= [10, 10],\n",
    "        size=[nsamples, 2],\n",
    "    )).double()\n",
    "\n",
    "    return s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - A-DPPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - Unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_pe = 100\n",
    "n_rho = 1_000\n",
    "n_roll = 400\n",
    "\n",
    "alpha = 1.0\n",
    "eta = 0.0001\n",
    "\n",
    "env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "dpgpd = ADpgpdSampled(ds, da, env, eta, tau, gamma, b, alpha, primal_reward_fn, primal_reward_reg_fn, dual_reward_fn, starting_pos_fn)\n",
    "\n",
    "K, losses_primal, losses_dual = dpgpd.train_unconstrained(epochs, n_pe, n_rho, n_roll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B - Constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40_000\n",
    "n_pe = 100\n",
    "n_rho = 1_000\n",
    "n_roll = 400\n",
    "\n",
    "alpha = 1.0\n",
    "eta = 0.00001\n",
    "\n",
    "env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "dpgpd = ADpgpdSampled(ds, da, env, eta, tau, gamma, b, alpha, primal_reward_fn, primal_reward_reg_fn, dual_reward_fn, starting_pos_fn)\n",
    "\n",
    "K, lmbda, losses_primal, losses_dual = dpgpd.train_constrained(epochs, n_pe, n_rho, n_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../results/vel_sampled_primal.npy', losses_primal)\n",
    "np.save('../results/vel_sampled_dual.npy', losses_dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - PGDual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40_000\n",
    "n_samples = 100\n",
    "n_rollout = 400\n",
    "n_rho = 1_000\n",
    "\n",
    "n_dual_update = 100\n",
    "lr_actor = 1e-4\n",
    "lr_dual = 1e-5\n",
    "\n",
    "env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "\n",
    "ld = LinearDual(ds, da, env, lr_actor, lr_dual, gamma, b, starting_pos_fn, primal_reward_fn, dual_reward_fn)\n",
    "loss_primal, loss_dual = ld.train(n_epochs, n_samples, n_rollout, n_rho, n_dual_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    n_epochs = 40_000\n",
    "    n_samples = 100\n",
    "    n_rollout = 400\n",
    "    n_rho = 1_000\n",
    "\n",
    "    n_dual_update = 100\n",
    "    lr_actor = 1e-4\n",
    "    lr_dual = 1e-5\n",
    "\n",
    "    env = RobotWorld(range_pos=[40, 50], range_vel=[-.1, .1])\n",
    "\n",
    "    ld = LinearDual(ds, da, env, lr_actor, lr_dual, gamma, b, starting_pos_fn, primal_reward_fn, dual_reward_fn)\n",
    "    losses_primal, losses_dual = ld.train(n_epochs, n_samples, n_rollout, n_rho, n_dual_update)\n",
    "    return K, lmbda, losses_primal, losses_dual\n",
    "\n",
    "def run_parallel_experiments(n_experiments):\n",
    "    seeds = [np.random.randint(0, 1000000) for _ in range(n_experiments)]\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(run_experiment, seeds)\n",
    "    return results\n",
    "\n",
    "n_experiments = 10\n",
    "results = run_parallel_experiments(n_experiments)\n",
    "\n",
    "Ks, lambdas, losses_primals, losses_duals = zip(*results)\n",
    "avg_K = np.mean(Ks, axis=0)\n",
    "avg_lambda = np.mean(lambdas, axis=0)\n",
    "avg_losses_primal = np.mean(losses_primals, axis=0)\n",
    "avg_losses_dual = np.mean(losses_duals, axis=0)\n",
    "\n",
    "print(\"Average K:\", avg_K)\n",
    "print(\"Average lambda:\", avg_lambda)\n",
    "print(\"Average primal losses:\", avg_losses_primal)\n",
    "print(\"Average dual losses:\", avg_losses_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../results/vel_sampled_primal_dm.npy', loss_primal)\n",
    "np.save('../results/vel_sampled_dual_dm.npy', loss_dual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
